- ## Non-linear #[[Non-linear]]
	- Deep Learning 的模型需要將非線性函數（激活函數, activation function）穿插在線性運算（e.g. Linear Layer, Convolutional layer）之間，建構的模型才能解決非線性問題
- ## Deeper or Wider
	- ### Deeper
		- 在 [[AI&ML&DL]] 提到了 $f_{\phi}$ 是由數層的 Neural Network 所建構
			- 在每層逐步提取不同的 feature vector，可以學習不同層次的抽象特徵，能使其具有更佳的泛化能力
			  id:: 62ee231a-a676-43d6-8a43-55a1820c9e11
	- ### Wider
		- 理論上只要有 **足夠** 的資料，便可以靠 **非常寬**、只有一層隱藏層的 [[Multilayer Perceptron]] (MLP) 擬合任何的函數
			- 然而 wide neural network 的泛化能力低落，只是將資料記憶在網路中而已
			- 需要 **非常大量** 的資料與參數才能讓 wide neural network 擁有與 deep neural network 相等的能力
	- 相關研究可參考
		- [Why Deep Neural Networks for Function Approximation?](https://arxiv.org/abs/1610.04161)
		- [Universal approximation theorem](https://en.m.wikipedia.org/wiki/Universal_approximation_theorem)
- ## 訓練流程
	- ![training-process.png](../assets/training-process.png){width 500}
	- ### Forward #Architectures
		-
	- ### Loss #Loss
	- ### Backward
	- ### Optimize #Optimization
- [[Supervised]]
	- y 是人為標記的 label
- [[Unsupervised]]
	- 沒有人為標記的 y
- [[Semi-Supervised]]
	- 使用人工標記與機器標記兩種資料進行訓練
- Pretrain & Transfer Learning
	-